Today: Backups (Ops people worry about), Version control (Devs worry about)

# History of Git 

### What is a version control? 

## The different needs of backups and version control

**Version control needs**: 
- recoever from screwup -- fancy undo (undo something from last week, or a whole set of things, etc). 
- explore history: answer the *why* questions. Why is it the way it is? Why not another way? An altarnative tech for comments. 

**Backup needs**: 
- undo massive data changes 
- Motivated by being able to respond to failure scenerios
    
    - you lose power
    - flash drive fails (some or none is recoverable)
    - user deletes/ trashes files by mistake 
    - an outside attacker deletes/trashes files (ransomeware)
    - *the hardest problem* inside attacker doing teh same thing -- because he could trash your backups. *we will not consider this*. 

### Statistics to decide what to respond to: Annularize failure rate (AFR)
Prob that drive fails in a year of uses (use quantification somewhat subjective) 
The precentege is .2%-2.5% per year. 

## What are some possible backup strategies? 
strats: 
### RAID-1 *mirroring*:
 Have 2 drives having the same data, prob both fail is tiny. not cost affective. (relatively) Esay to implement. Proce scales linearly with storage. The combined AFR $\neq{}(AFR)^2$ because the events are not independent. But it's also a passimastic estimate because you replace bad drives. Operations stadd person will make these calculations routinely.
**GO BACK TO LECTURE THERE WAS SOME QUESTION ABOUT THE PROB OF FAILURE I DIDN'T HEAR I SHOULD KNOW BEFORE THE EXAM**
- mirroring only helps with recovering this moments data. If teh user deletes something, he will have deleted it from the disk and it will be recoverable again. 

### To proceed, *we must ask: what do we need to back up?*: 

    - file contents 
    - filesystem data (things other than contents -- metadata (the stuff from `ls -l`))
    - `df -h` filesystem orginisation, includes things like partiotions (how we divide data), mount points (where the filesystems apear to the users), options (read only --ro). System config information that is too low level to be in a reg ol' file. 
    - firmware (when you boot, the system does not run any code from a file, it runs "code" from hard-coded memory blocks). 

The first 2 are our focus 

### Two approaches: the first is *block level*
You don't care what blocks are being used for, just back up blocks of mem. can optimize at the block level. 

More simple, more general, applicable to all fs.

### Two approaches: Filesystem level 
We can backup more inteligently based on knowing how the filesystem works. For example change of file must also change the timestamp. 

More likely to be efficient, specific to fs, more difficult to create. 

#### What changes can we ignore when doing backups? 
- don't bother backing up unused data
- don't backup caches and other reconstructable/preformence-oriented data. (speeds up backups, but slows down recovery). 


#### When do you reclaim backup storage
you backup every x time, what do you do when you run out of storage? The dumb strategy is to run them out linearly. The other is to sparse them out like: 
    - 1/day for last 6 months
    - 1/week for last 6 years
    - ...

#### How do you do backups (more) cheaply? 
- simpleton: Do them less often.
- Let's do backups to a cheaper device (main is fast flash drive, the backup is older disk drive, mag tape, optical). Can create a chain of cheaper and cheaper backups for less and less accessed stuff. 
- Remote backups -- issue for privacy and trust. 
- do them incrementlly: Only save teh blocks which changed, or be even fancier and save only changes of files that changed (diff file/ $\delta{}$)

    - how do we express delta? 
    
        - in blocks: `[(indexes,new values)]`
        - for text file (filesystem level) backup you use an edit script. SO like a file of a little language that can be ran to change the lines of a file (commands like append, delete, replace, etc... can be more complicated to optimise). Fancier scripting langugae = smaller backups , but less efficient reconstruction. 

- Deduplication (at the block level): Look at all blocks in you system, some blocks will be very simillar or identical -- we don't need an individual backup for each! Now instead of just blocks, we have (block, [list of indexes of indentical data]). Can be combined with deltas to make it work even for simillar blocks. More subject to problems if the drive fails (has partial failures). In practiacal systems, there's a lower level system that prevents failures (like mirroring). Very common in high-end storage systems. 

## How do you know your backups are working? 
- unplug a random drive in the server room, that is, test them as realistically as possible. That's expensive. 
- cheaper alternatives -- checksums. save checksums of the way the data used to be, then use checksums (hashs, approximately) that allows you to see if what you pulled is what you put in. 


# Versioning at the file level (OS help)
- files can have versions, for exampele (not fr but could be) `ls -l --all-versions` (you will see file;19 -- saying that it's the 19'th verison) and so on until myfile versopn1. Rarely used nowdays because: 

    - how do we decide when to increment the file version? EVery write? too much. Instead, let the application control when a new version is created. 
    - how to prune (what is prune?) older versions
    - some application will not work well with this system. Like a DatabaseManagemn=entSys. 

- Snapshots: System decides, not applications, when to take a snapshot of every file in the system. (butfs, zfs, WAFL)

    - in WAFL -- .snapshots sunbdirectory in every snapshots. Not perfect, to , let's say, 15min of work lost. can't delete snapchots. 
    - soudsn slow, but itsn't because of *copy-on-write*
    - a copy of the filesystem doesn't take very long becuase we just have the copy one write. 
    - that's what WAFL does and that's why it fast. $whuss\; the\; deal$

### copy-on-write (CoW)
`$ cp --reflink=always a b` let b just point to a. Only 1 copy of the file. We mark a and b that these files is just sharing data. Once you modify b block something, the sytsem will create a new vlock for b and then change it. Like a smart point by reference. (lazy copying )

### WAFL filesystem 
*read up on this, lowkey spaced out a d bit and it seems important and interesting*
root points to everything reachable from the root in a tree structure. We differ the work the until we chnage and actually need to. The file system that we go to when we cs on wafl is a tree from another root. Not allowed to change any of the .snapshot tree.

We can reclain all the blocks that are not always reachable. 


## Software Version COnstrol systems (assuming files work)
control: 
- source code
- test cases
- documentation
- sales blurbs

(common) Limits:
- big binary blobs (such as vids or imgs).

    - in practice use version control for the stuff it works, then a list of names of blobs for the rest of the stuff wheer teh blobs are kepts elsewhere. 

**WHat do we want? in our vc system?* 
- histories (can't keep snapshots indefinetly, in sw systems we want those systems to be indefinite). Histories contain metaadta as well as data. Metadata is about filesystem and about the history iteself (who made this commit?)
- when file changed names;; What to do abotu filerenames? (When you do a replace you lose the fact that it's really the same file). Systems that model with delete + insert may not get that. 
- last 3 changes made? (meta meta information) *know this for final*
- atomic commits to multiple files -- change all or change none! More difficult than in fs because in fs we do atomic changes to files.  
- hooks -- alter places in VCD that the user can can execute arbitary code. *git doesn't do exactly what you want, often too permissive* restrict changes per project. 
- security (but we again don't talk abotu it) how do you know last commit was done by fellow teamate and not by attacker? Auth changes. 
- format conversion (CRLF vs LF, for example). 
- encodings (UTF-8 vs lesser encodings) -- want everyone to be able to wirk with the same encoding/their encoding 
- navigation of complex systems (graphical tool or better)


### history of version control 
from (X -> Git (2002)). 

git came out revolting against BitKeeeper (which was propriety code for Linux). Git said let's do the ideas of bitKeeper from scratch, but do it open source! Licensing was the big deal. 

- SCCS Bell labs ~ 1975 (single file vc -- every file had it's own repository) -- the design choice from here that survived is that you can walk through the repository in a single walk and retrieve it any version that you would like. (delta backwards and forward). Can save less and, but O(size of rep to commit from repo)
- RCS (file only) -- save later version O (latest version) for retrievel. **How efficient will it be to grab what I want?**
- CVS (centrakl repositoy )
- Subversion (no central repo)
- Bitkeeper (let's do atomic commits and have no central repository)
